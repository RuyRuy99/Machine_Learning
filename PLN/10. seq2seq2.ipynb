{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1tVx3TXs6Kz0YSMnkBv7notWKnxKaDok9","timestamp":1675989210159}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# Based on https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb\n","\n","# Install dependencies\n","!pip install torchdata\n","!pip install torchinfo\n","!spacy download en_core_web_sm\n","!spacy download de_core_news_sm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5THBzmCurbe_","outputId":"9da05dd5-1c6f-4448-dd75-7e2095edc1e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchdata\n","  Downloading torchdata-0.4.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n","\u001b[K     |████████████████████████████████| 4.4 MB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.12.1+cu113)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.23.0)\n","Collecting urllib3>=1.25\n","  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n","\u001b[K     |████████████████████████████████| 140 kB 72.6 MB/s \n","\u001b[?25hCollecting portalocker>=2.0.0\n","  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.12.1->torchdata) (4.1.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (3.0.4)\n","Collecting urllib3>=1.25\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 51.2 MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2.10)\n","Installing collected packages: urllib3, portalocker, torchdata\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed portalocker-2.5.1 torchdata-0.4.1 urllib3-1.25.11\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchinfo\n","  Downloading torchinfo-1.7.1-py3-none-any.whl (22 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.7.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting en-core-web-sm==3.4.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n","\u001b[K     |████████████████████████████████| 12.8 MB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.0) (3.4.1)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.7)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.3)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.23.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.2)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.1.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.1)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (57.4.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.21.6)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.25.11)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.0.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting de-core-news-sm==3.4.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.4.0/de_core_news_sm-3.4.0-py3-none-any.whl (14.6 MB)\n","\u001b[K     |████████████████████████████████| 14.6 MB 1.8 MB/s \n","\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from de-core-news-sm==3.4.0) (3.4.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.10)\n","Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (4.1.1)\n","Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.10.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.21.6)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.8)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.0.8)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (8.1.3)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (4.64.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.3.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (57.4.0)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.6.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (21.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.4.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.11.3)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.6)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.7)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.9.2)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.0.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.23.0)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.4.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.9)\n","Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (5.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (1.25.11)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.10)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.7.8)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (0.0.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (7.1.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->de-core-news-sm==3.4.0) (2.0.1)\n","Installing collected packages: de-core-news-sm\n","Successfully installed de-core-news-sm-3.4.0\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('de_core_news_sm')\n"]}]},{"cell_type":"code","source":["# Mount drive to save/load model\n","\n","from google.colab import drive\n","drive.mount('/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UizR8-ZFHxzI","outputId":"5db09c5d-0858-4165-cb33-d93260d225d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}]},{"cell_type":"markdown","source":["# Preparing Data"],"metadata":{"id":"f7xO25UznUVu"}},{"cell_type":"code","source":["# Importing 2 tokenizers (English and German) from spacy \n","\n","import torchtext\n","\n","tokenizer_de = torchtext.data.utils.get_tokenizer('spacy', language='de_core_news_sm')\n","tokenizer_en = torchtext.data.utils.get_tokenizer('spacy', language='en_core_web_sm')"],"metadata":{"id":"dLizmxiyniFn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load dataset. \n","# English, German pairs\n","\n","from torchtext.datasets import Multi30k\n","\n","train_data = Multi30k(split='train')\n","valid_data = Multi30k(split='valid')\n","test_data = Multi30k(split='test')"],"metadata":{"id":"7GDm1qXhpP6c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check one example of the dataset\n","\n","for i in train_data:\n","  print(i)\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xqZGCKsVrHzj","outputId":"b70c17f7-8e60-4c5e-cdf9-837ce4aa4c04"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["('Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.', 'Two young, White males are outside near many bushes.')\n"]}]},{"cell_type":"code","source":["# Build vocab and add <sos> and <eos> special tokens\n","\n","def de_tokens(data_iter):\n","  for de_text, en_text in data_iter:\n","    yield tokenizer_de(de_text.lower())\n","\n","vocab_de = torchtext.vocab.build_vocab_from_iterator(de_tokens(train_data), specials=['<sos>', '<eos>'], min_freq=2)"],"metadata":{"id":"G4Pt0lcussp3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check the vocabulary\n","\n","print(len(vocab_de))\n","print(vocab_de.get_itos()[:10])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cr6VWOfztPAg","outputId":"455d706e-4f26-4ba2-891e-cd51daf37129"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["7851\n","['<sos>', '<eos>', '.', 'ein', 'einem', 'in', 'eine', ',', 'und', 'mit']\n"]}]},{"cell_type":"code","source":["# Build vocab and add <sos> and <eos> special tokens\n","\n","def en_tokens(data_iter):\n","  for de_text, en_text in data_iter:\n","    yield tokenizer_en(en_text.lower())\n","\n","vocab_en = torchtext.vocab.build_vocab_from_iterator(en_tokens(train_data), specials=['<sos>', '<eos>'], min_freq=2)"],"metadata":{"id":"x5AiDIQQtUbq","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e3e34d93-c8ad-44b2-a45f-1545c84ffbf7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/datapipes/iter/combining.py:249: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n","  \"the buffer and each child DataPipe will read from the start again.\", UserWarning)\n"]}]},{"cell_type":"code","source":["# Check the vocabulary\n","\n","print(len(vocab_en))\n","print(vocab_en.get_itos()[:10])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NOD1rmfkutnY","outputId":"ad81efd9-9203-409b-cebf-a79526e7c883"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["5891\n","['<sos>', '<eos>', 'a', '.', 'in', 'the', 'on', 'man', 'is', 'and']\n"]}]},{"cell_type":"code","source":["# Import torch and get the device\n","\n","import torch\n","from torch import nn\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"bKmD0JEWuu1K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Dataset / Dataloader"],"metadata":{"id":"SYZsmUz8vfAZ"}},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader"],"metadata":{"id":"Fny9qoe9v8XH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TranslationDataset(Dataset):\n","  def __init__(self, dataset, vocab_de, vocab_en):\n","    self.dataset = dataset\n","    self.data_en = []\n","    self.data_de = []\n","\n","    for de_text, en_text in dataset:\n","      # tokenize and add special tokens\n","      tokens_en = ['<sos>'] + tokenizer_en(en_text.lower()) + ['<eos>']\n","      tokens_de = ['<sos>'] + tokenizer_de(de_text.lower()) + ['<eos>']\n","\n","      # filter tokens to use only tokens in the vocabulary\n","      tokens_en = [[vocab_en[token]] for token in tokens_en if token in vocab_en]\n","      tokens_de = [[vocab_de[token]] for token in tokens_de if token in vocab_de]\n","      \n","      self.data_en.append(tokens_en)\n","      self.data_de.append(tokens_de)\n","\n","  def __len__(self):\n","    return len(self.data_en) - 1\n","\n","  def __getitem__(self, idx):\n","    return torch.LongTensor(self.data_en[idx]), torch.LongTensor(self.data_de[idx])\n","           \n"],"metadata":{"id":"9FDYkABvv7C4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check one example of the dataset\n","\n","train_dataset = TranslationDataset(train_data, vocab_de, vocab_en)\n","train_dataset[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WEdqTwfQwyAz","outputId":"359507f3-715f-4732-efdd-87526a524e03"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[   0],\n","         [  14],\n","         [  22],\n","         [  13],\n","         [  23],\n","         [ 776],\n","         [  15],\n","         [  55],\n","         [  78],\n","         [ 200],\n","         [1310],\n","         [   3],\n","         [   1]]), tensor([[   0],\n","         [  16],\n","         [  24],\n","         [ 251],\n","         [  28],\n","         [  82],\n","         [  18],\n","         [  86],\n","         [   5],\n","         [  13],\n","         [ 108],\n","         [7645],\n","         [3169],\n","         [   2],\n","         [   1]]))"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["# Create dataloaders\n","# Using batch size 1 for simplicity\n","\n","BATCH_SIZE = 1\n","dataloader_train = DataLoader(TranslationDataset(train_data, vocab_de, vocab_en), batch_size=BATCH_SIZE)\n","dataloader_valid = DataLoader(TranslationDataset(valid_data, vocab_de, vocab_en), batch_size=BATCH_SIZE)"],"metadata":{"id":"YMkQFvXIvQJX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check dataloader length\n","\n","len(dataloader_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9_cdY0ca604H","outputId":"1a1b9f71-6e8f-4b00-b5cf-120567192844"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["29000"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["# Network"],"metadata":{"id":"REglJAlUziDq"}},{"cell_type":"code","source":["from torch import nn\n","\n","class Encoder(nn.Module):\n","  def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n","    super().__init__()\n","\n","    self.hid_dim = hid_dim\n","    self.n_layers = n_layers\n","\n","    self.embedding = nn.Embedding(input_dim, emb_dim)\n","    self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, src):\n","    embedded = self.dropout(self.embedding(src))\n","    outputs, (hidden, cell) = self.rnn(embedded)\n","\n","    return hidden, cell"],"metadata":{"id":"fY6yekinvwg7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchinfo import summary\n","\n","INPUT_DIM = len(vocab_en)\n","ENC_EMB_DIM = 128\n","HID_DIM = 64\n","N_LAYERS = 1\n","ENC_DROPOUT = 0.3\n","\n","enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT).to(device)\n","\n","summary(enc, input_size=(1,), dtypes=[torch.long])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4PemdPvqo0Bw","outputId":"edb0a78f-467f-4349-999d-4e66ac887cea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"]},{"output_type":"execute_result","data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","Encoder                                  [1, 64]                   --\n","├─Embedding: 1-1                         [1, 128]                  754,048\n","├─Dropout: 1-2                           [1, 128]                  --\n","├─LSTM: 1-3                              [1, 64]                   49,664\n","==========================================================================================\n","Total params: 803,712\n","Trainable params: 803,712\n","Non-trainable params: 0\n","Total mult-adds (M): 3.93\n","==========================================================================================\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.00\n","Params size (MB): 3.21\n","Estimated Total Size (MB): 3.22\n","=========================================================================================="]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["class Decoder(nn.Module):\n","  def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n","    super().__init__()\n","    self.output_dim = output_dim\n","    self.hid_dim = hid_dim\n","    self.n_layers = n_layers\n","\n","    self.embedding = nn.Embedding(output_dim, emb_dim)\n","    self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n","    \n","    # need to return a layer with size output_dim\n","    self.fc_out = nn.Linear(hid_dim, output_dim)\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, input, hidden, cell):\n","    input = input.unsqueeze(0)  # add one dimension\n","    embedded = self.dropout(self.embedding(input))\n","    output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n","    prediction = self.fc_out(output.squeeze(0))  # remove dimensions with size 1\n","\n","    return prediction, hidden, cell"],"metadata":{"id":"mSEvm_1h0jsm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["OUTPUT_DIM = len(vocab_de)\n","DEC_EMB_DIM = 128\n","HID_DIM = 64\n","N_LAYERS = 1\n","DEC_DROPOUT = 0.3\n","\n","dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT).to(device)\n","\n","summary(dec)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VcwmyUXIsqbt","outputId":"c08a0402-abfc-4d93-b9bb-5dc5d0b298a0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"]},{"output_type":"execute_result","data":{"text/plain":["=================================================================\n","Layer (type:depth-idx)                   Param #\n","=================================================================\n","Decoder                                  --\n","├─Embedding: 1-1                         1,004,928\n","├─LSTM: 1-2                              49,664\n","├─Linear: 1-3                            510,315\n","├─Dropout: 1-4                           --\n","=================================================================\n","Total params: 1,564,907\n","Trainable params: 1,564,907\n","Non-trainable params: 0\n","================================================================="]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["import random\n","\n","class Seq2Seq(nn.Module):\n","  def __init__(self, encoder, decoder, device):\n","    super().__init__()\n","\n","    self.encoder = encoder\n","    self.decoder = decoder\n","    self.device = device\n","\n","    # encoder hidden is passed to decoder hidden, so it. must be equal\n","    assert encoder.hid_dim == decoder.hid_dim, \\\n","      \"Hidden dimensions of encoder and decoder must be equal!\"\n","    assert encoder.n_layers == decoder.n_layers, \\\n","      \"Encoder and decoder must have equal number of layers!\"\n","\n","  def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n","    batch_size = 1  # using batch size 1 for simplicity\n","    trg_len = trg.shape[0]  # the number of tokens of the target\n","    trg_vocab_size = self.decoder.output_dim\n","\n","    # tensor to store decoder outputs\n","    # outputs has the size according the target\n","    outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n","\n","    #last hidden state of the encoder is used as the initial hidden state of the decoder\n","    hidden, cell = self.encoder(src)\n","\n","    #first input to the decoder is the <sos> tokens\n","    input = trg[0,:]\n","\n","    # iterate over the target len\n","    for t in range(1, trg_len):\n","      #insert input token embedding, previous hidden and previous cell states\n","      #receive output tensor (predictions) and new hidden and cell states\n","      output, hidden, cell = self.decoder(input, hidden, cell)\n","      # print('seq2seq output', output)\n","      # print('seq2seq output arg', output.argmax(1))\n","\n","      #place predictions in a tensor holding predictions for each token\n","      outputs[t] = output\n","\n","      #get the highest predicted token from our predictions\n","      top1 = output.argmax(1) \n","\n","      #decide if we are going to use teacher forcing or not\n","      teacher_force = random.random() < teacher_forcing_ratio\n","            \n","      #if teacher forcing, use actual next token as next input\n","      #if not, use predicted token\n","      input = trg[t] if teacher_force else top1\n","        \n","    return outputs\n"],"metadata":{"id":"PScvrlt21rYg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"NA9IT09s3R1J"}},{"cell_type":"code","source":["model = Seq2Seq(enc, dec, device).to(device)"],"metadata":{"id":"112G7JXj3MdN","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ddc15243-5797-4410-c5d7-7ad8f9270988"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n"]}]},{"cell_type":"code","source":["def init_weights(m):\n","  for name, param in m.named_parameters():\n","    nn.init.uniform_(param.data, -0.08, 0.08)\n","        \n","model.apply(init_weights)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MDZ-gqjiolMi","outputId":"cda3783e-20db-4046-9f17-616c9cc621d2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Seq2Seq(\n","  (encoder): Encoder(\n","    (embedding): Embedding(5891, 128)\n","    (rnn): LSTM(128, 64, dropout=0.3)\n","    (dropout): Dropout(p=0.3, inplace=False)\n","  )\n","  (decoder): Decoder(\n","    (embedding): Embedding(7851, 128)\n","    (rnn): LSTM(128, 64, dropout=0.3)\n","    (fc_out): Linear(in_features=64, out_features=7851, bias=True)\n","    (dropout): Dropout(p=0.3, inplace=False)\n","  )\n",")"]},"metadata":{},"execution_count":112}]},{"cell_type":"code","source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UFpTkGVr3blE","outputId":"3d7d2316-cbbb-44e9-8232-1088a18451a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The model has 2,368,619 trainable parameters\n"]}]},{"cell_type":"code","source":["from torch import optim\n","\n","optimizer = optim.Adam(model.parameters())\n","criterion = nn.CrossEntropyLoss()"],"metadata":{"id":"__KPoXaq32mA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, dataloader, optimizer, criterion, clip=1):\n","  model.train()\n","  epoch_loss = 0\n","\n","  for src, trg in dataloader:\n","    src, trg = src.to(device)[0], trg.to(device)[0]\n","\n","    optimizer.zero_grad()\n","\n","    output = model(src, trg)\n","\n","    # remove first token and format flat the tensors\n","    output = output[1:].view(-1, OUTPUT_DIM)\n","    trg = trg[1:].view(-1)\n","\n","    loss = criterion(output, trg)\n","\n","    loss.backward()\n","\n","    # torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","    optimizer.step()\n","    epoch_loss += loss.item()\n","\n","  return epoch_loss / len(dataloader)\n"],"metadata":{"id":"hRjm9Yck4C5V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(model, dataloader, criterion):\n","  model.eval()\n","  epoch_loss = 0\n","\n","  with torch.no_grad():\n","    for src, trg in dataloader:\n","      src, trg = src.to(device)[0], trg.to(device)[0]\n","      output = model(src, trg, 0)\n","\n","      output = output[1:].view(-1, OUTPUT_DIM)\n","      trg = trg[1:].view(-1)\n","      \n","      loss = criterion(output, trg)\n","      epoch_loss += loss.item()\n","\n","  return epoch_loss / len(dataloader)"],"metadata":{"id":"w4cR2jlx6FNW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(dataloader_train), len(dataloader_valid)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a6Yaeb6m9DXq","outputId":"89c8e9d1-1607-4f29-c72e-662703a359a9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(29000, 1014)"]},"metadata":{},"execution_count":117}]},{"cell_type":"code","source":["def epoch_time(start_time, end_time):\n","  elapsed_time = end_time - start_time\n","  elapsed_mins = int(elapsed_time / 60)\n","  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","  return elapsed_mins, elapsed_secs"],"metadata":{"id":"cB7CZMfw6A3Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_de.get_itos()[1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"mv8ZdbHi8NL3","outputId":"99842484-15cc-4d63-a8e2-8a32f215b6d3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<eos>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":119}]},{"cell_type":"code","source":["import time\n","import math\n","\n","N_EPOCHS = 10\n","\n","for epoch in range(N_EPOCHS):\n","  start_time = time.time()\n","  train_loss = train(model, dataloader_train, optimizer, criterion)\n","  valid_loss = evaluate(model, dataloader_valid, criterion)\n","  end_time = time.time()\n","\n","  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","  print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","  print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n","  print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qV0fmFNV6C4q","outputId":"23777a1e-3105-456c-ed35-1a3755cc4167"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 01 | Time: 6m 16s\n","\tTrain Loss: 4.421 | Train PPL:  83.208\n","\t Val. Loss: 4.468 |  Val. PPL:  87.194\n","Epoch: 02 | Time: 6m 16s\n","\tTrain Loss: 3.773 | Train PPL:  43.498\n","\t Val. Loss: 4.341 |  Val. PPL:  76.755\n","Epoch: 03 | Time: 6m 16s\n","\tTrain Loss: 3.560 | Train PPL:  35.155\n","\t Val. Loss: 4.210 |  Val. PPL:  67.327\n","Epoch: 04 | Time: 6m 11s\n","\tTrain Loss: 3.428 | Train PPL:  30.808\n","\t Val. Loss: 4.151 |  Val. PPL:  63.528\n","Epoch: 05 | Time: 6m 12s\n","\tTrain Loss: 3.324 | Train PPL:  27.766\n","\t Val. Loss: 4.129 |  Val. PPL:  62.100\n","Epoch: 06 | Time: 6m 11s\n","\tTrain Loss: 3.245 | Train PPL:  25.669\n","\t Val. Loss: 4.079 |  Val. PPL:  59.098\n","Epoch: 07 | Time: 6m 11s\n","\tTrain Loss: 3.186 | Train PPL:  24.194\n","\t Val. Loss: 4.001 |  Val. PPL:  54.660\n","Epoch: 08 | Time: 6m 11s\n","\tTrain Loss: 3.137 | Train PPL:  23.033\n","\t Val. Loss: 3.983 |  Val. PPL:  53.701\n","Epoch: 09 | Time: 6m 11s\n","\tTrain Loss: 3.096 | Train PPL:  22.112\n","\t Val. Loss: 4.005 |  Val. PPL:  54.870\n","Epoch: 10 | Time: 6m 10s\n","\tTrain Loss: 3.066 | Train PPL:  21.450\n","\t Val. Loss: 3.957 |  Val. PPL:  52.303\n"]}]},{"cell_type":"code","source":["torch.save(model, '/gdrive/MyDrive/models/seq2seq.pt')"],"metadata":{"id":"NpQJn2GWH31m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataloader_test = DataLoader(TranslationDataset(test_data, vocab_de, vocab_en), batch_size=BATCH_SIZE)"],"metadata":{"id":"oGX472nDcmrh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_loss = evaluate(model, dataloader_test, criterion)\n","\n","print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gKypHUi17NAG","outputId":"dd8f715d-94b1-4118-e65a-b98e58de8eb7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["| Test Loss: 3.909 | Test PPL:  49.827 |\n"]}]},{"cell_type":"markdown","source":["# Translate"],"metadata":{"id":"VRzXiUgacx1B"}},{"cell_type":"code","source":["model = torch.load('/gdrive/MyDrive/models/seq2seq.pt')"],"metadata":{"id":"WA9jV-JtmP8S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def to_en_text(x):\n","  return ' '.join([vocab_en.get_itos()[token.item()] for token in x.flatten()])\n","\n","def to_de_text(x):\n","  return ' '.join([vocab_de.get_itos()[token.item()] for token in x.flatten()])"],"metadata":{"id":"D-6oMeiidQT2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dataset = TranslationDataset(test_data, vocab_de, vocab_en)"],"metadata":{"id":"pZ7rBkaXDHwY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["xx = test_dataset[12][0]\n","to_en_text(xx)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"JZrd4BrpdN8Y","outputId":"08566b2b-a961-43f7-c5bd-9556b46ced37"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<sos> a woman holding a bowl of food in a kitchen . <eos>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":140}]},{"cell_type":"code","source":["xx = test_dataset[12][1]\n","to_de_text(xx)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"kbm7QBI_dbI3","outputId":"a62a702c-1103-49f2-af8f-41e8c691713e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<sos> eine frau , die in einer küche eine schale mit essen hält . <eos>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":141}]},{"cell_type":"code","source":[],"metadata":{"id":"gz4C189Ne3Jd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","  src = test_dataset[12][0]\n","  trg = torch.tensor([0])\n","  src, trg = src.to(device), trg.to(device)\n","  \n","  print('src', src.shape)\n","  hidden, cell = enc(src)\n","\n","  print('hidden', hidden.shape)\n","  print('cell', cell.shape)\n","  print('trg', trg.shape)\n","  \n","  result = []\n","  for i in range(50):\n","    output, hidden, cell = dec(trg, hidden, cell)\n","    trg = output.argmax(1)\n","    result.append(trg)\n","\n","    if trg.item() == 1:  # eos\n","      break\n","\n","to_de_text(torch.tensor(result))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":140},"id":"ECUGuMr9eKkQ","outputId":"4f7f4fee-dbfe-4ba0-fb0c-ecffab694295"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["src torch.Size([13, 1])\n","hidden torch.Size([1, 1, 64])\n","cell torch.Size([1, 1, 64])\n","trg torch.Size([1])\n"]},{"output_type":"execute_result","data":{"text/plain":["'eine frau hält eine in einer küche und essen . <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos> <eos>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":142}]},{"cell_type":"code","source":["<sos> eine frau , die in einer küche eine schale mit essen hält . <eos>"],"metadata":{"id":"5ZHMcDgujEtL"},"execution_count":null,"outputs":[]}]}